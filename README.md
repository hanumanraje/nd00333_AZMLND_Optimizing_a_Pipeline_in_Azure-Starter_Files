# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This dataset contains data about bank telemarketing. The dataset has 21 columns and about 32000 rows. We're trying to predict the column named 'y' which has two class possibilities (yes, no). There is heavy class imbalance in the dataset. There are less than 3200 'yes' records which accounts to around 10% of the total rows.


The best performing model was the VotingEnsemble model built by AutoML. The accuracy of this model was 91.68%

## Scikit-learn Pipeline

The training pipeline for SKLearn model is as follows:
* Load the data from TabularDatasetFactory
* Clean the dataset using predefined function named **clean_data**
  * One Hot Encode the categorical columns
  * Separate the features and target
* Split the dataset into train and test sets
* Build a Logistic Regression model
* Tune hyperparameters using Hyperdrive
* Select the best model from the tuning
* Save and register the model for the later use 

Random Sampling to choose the hyperparameters. This type of sampling is good for initial experiments and we can refine our search space in later experiments. 

Random Sampling
* Supports discrete and continuous hyperparameters
* Supports early termination of low performance runs


I choose Bandit Policy for early termination. This policy takes **slack_factor or slack_amount** as an input and any run that doesn't fall within the slack factor or slack amount of the evaluation metric with respect to the best performing run will be terminated.

Example:

Consider a Bandit policy with slack_amount = 0.1 and evaluation_interval = 100. If Run 3 is the currently best performing run with an AUC (performance metric) of 0.8 after 100 intervals, then any run with an AUC less than 0.7 (0.8 - 0.1) after 100 iterations will be terminated. Similarly, the delay_evaluation can also be used to delay the first termination policy evaluation for a specific number of sequences.

## AutoML

The best model generated by AutoML is a VotingEnsemble classifier. This is built using engineered features and used LightGBM as the model. This model gave the best accuracy of 91.7% among all the other models generated. Before the model building, the AutoML also performed some data checks, where it found the class imbalance in our dataset. It balanced the dataset before going to model building.

The pipeline for the best model is as follows:

* Separate Numerical and Categorical columns
* Impute all the numerical columns with the mean value of each column
* The categorical columns were then Label Encoded and Vectorized using Count Vectorizer
* Then that dataset is sent to the model for training



## Pipeline comparison

The Voting Ensemble model outperform the Logistic Regression one with an accuracy of 0.9181 against 0.9129.

The differences in architecture are huge:
1. HyperDrive starts multiple runs each of which trains the LogisticRegression model using different tuples of hyper-parameters
2. AutoML starts multiple runs each of which executes complex pipelines with choices of hyper-parameters, models and configuration details

## Future work

As the dataset is not complex and a linear model like Logistic is able to learn the data and give similar scores to complex ensemble models, we can work on improving this model. In terms os data preprocessing, treating class imbalance would be a major thing to try out and later we can experiment with different imputing techniques instead of mean (like nearest). There's probably some improvements that can be done in hyperparameters also. Giving a wider search space and diffrenet sampling technique like Bayesian Sampling.

As always, we can also use AutoML to comeup with the best model by giving it more time and compute resources. 

## Proof of cluster clean up

#delete the compute cluster

try:
    AmlCompute.delete(compute_target)
except:
    print('Can not delete the cluster!')

